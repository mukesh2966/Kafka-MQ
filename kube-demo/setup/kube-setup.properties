# download the kafka package kafka_2.13-2.8.0 link https://www.apache.org/dyn/closer.cgi?path=/kafka/2.8.0/kafka_2.13-2.8.0.tgz
#### USEFUL LINKS FOR THE IMAGES USED BELOW:
For zookeeper:
https://hub.docker.com/r/bitnami/zookeeper/

For broker:
https://github.com/bitnami/bitnami-docker-kafka/blob/master/README.md#setting-up-a-kafka-cluster
https://github.com/bitnami/bitnami-docker-kafka/blob/master/2/debian-10/rootfs/opt/bitnami/scripts/libkafka.sh#L221

#####_______________SUGGESTION
##### First go through the docker-setup.properties file, to get a better understanding of what's done below.


## launch all the 4 services, 3 brokers 1 zookeeper(corresponding yaml files could be found in pod-file folder)

# get their IP, perform reverse dns query on the IPS to get their corresponding hostname, which will be required in SSL keystore and truststore generation.

## For the secure setup on kubernetes, we would be using SASL_SSL for authentication, SSL for encryption and built-in ACL authorizer for authorization


###----------------------- For starting the zookeeper and brokers deployments on kubernetes--------------------

For starting with the setup on kubernetes, the idea is to first create the broker and zookeeper images that will be used.
Bitnami images with some mounted files will be used for this purpose.

### now what the idea is that instead of mounting the volumes like in docker-setup.
### we create new images from base bitnami images, with the required mounts already copied to the corresponding folders.

## These three things needed to be copied on top of base image for both zookeeper and broker
### certificates
### kafka_jaas.conf (only for brokers)
### server.properties and zookeeper.properties to the image
(For more detail you could check out the respective docker file inside the images folder)

Beforing mounting the above mentioned required files to base image, we first need to do the following steps:

##------------------------------------------------------------##
                   1. CREATE THE CERTIFICATES
##------------------------------------------------------------##


## to use SSL, first we need to create the required certificates 

Below are the Example steps to implement CA, truststore and keystore for zookeeper SSL security :
𝟏. 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐞 𝐂𝐀 == openssl req -new -x509 -keyout ca-key -out ca-cert -days 3650
𝟐. 𝐂𝐫𝐞𝐚𝐭𝐞 𝐓𝐫𝐮𝐬𝐭𝐬𝐭𝐨𝐫𝐞 == keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
𝟑. 𝐂𝐫𝐞𝐚𝐭𝐞 𝐊𝐞𝐲𝐬𝐭𝐨𝐫𝐞 == keytool -keystore zookeeper.keystore.jks -alias zookeeper -validity 3650 -genkey -keyalg RSA -ext SAN=dns:<dns-name>
𝟒. 𝐂𝐫𝐞𝐚𝐭𝐞 𝐜𝐞𝐫𝐭𝐢𝐟𝐢𝐜𝐚𝐭𝐞 𝐬𝐢𝐠𝐧𝐢𝐧𝐠 𝐫𝐞𝐪𝐮𝐞𝐬𝐭 (𝐂𝐒𝐑) == keytool -keystore zookeeper.keystore.jks -alias zookeeper -certreq -file ca-request-zookeeper
𝟓. 𝐒𝐢𝐠𝐧 𝐭𝐡𝐞 𝐂𝐒𝐑 == openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
𝟔. 𝐈𝐦𝐩𝐨𝐫𝐭 𝐭𝐡𝐞 𝐂𝐀 𝐢𝐧𝐭𝐨 𝐊𝐞𝐲𝐬𝐭𝐨𝐫𝐞 == keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
𝟕. 𝐈𝐦𝐩𝐨𝐫𝐭 𝐭𝐡𝐞 𝐬𝐢𝐠𝐧𝐞𝐝 𝐜𝐞𝐫𝐭𝐢𝐟𝐢𝐜𝐚𝐭𝐞 𝐢𝐧𝐭𝐨 𝐊𝐞𝐲𝐬𝐭𝐨𝐫𝐞 == keytool -keystore zookeeper.keystore.jks -alias zookeeper -import -file ca-signed-zookeeper

#### Suggestion : use the same password where-ever you are prompted for one. 
1. As in this setup multiple passwords will need to be generated. Why take extra headache to remember different passwords.
2. while creating the Certificate Authority, we can use any placeholder when asked for FQDN.
3. use domain name when prompted for first name and last name, while creating keystore

### First step is to create the Certificate Authority
openssl req -new -x509 -keyout ca-key -out ca-cert -days 3650

##------Moving on to create keystore and truststore for each component

##----------zookeeper
common name:155.188.68.34.bc.googleusercontent.com
ip:34.68.188.155

keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias zookeeper -validity 3650 -genkey -keyalg RSA -ext SAN=dns:155.188.68.34.bc.googleusercontent.com
keytool -keystore zookeeper.keystore.jks -alias zookeeper -certreq -file ca-request-zookeeper
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias zookeeper -import -file ca-signed-zookeeper


##-------Each of the brokers will have 2 pairs of certs(4 certs in total)
one pair of keystore and truststore to connect with zookeeper
another pair to connect with the broker-clients(other brokers, producer and cosumer)

##------------broker0

common name:234.67.66.34.bc.googleusercontent.com
ip:34.66.67.234

# for broker0 and zookeeper connections
keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:localhost
keytool -keystore zookeeper.keystore.jks -alias kafka -certreq -file ca-request-zookeeper
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -import -file ca-signed-zookeeper


# for broker-clients
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:234.67.66.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka


##------------broker1
common name:183.95.70.34.bc.googleusercontent.com
ip:34.70.95.183

# for broker1 and zookeeper connections
keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:localhost
keytool -keystore zookeeper.keystore.jks -alias kafka -certreq -file ca-request-zookeeper
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -import -file ca-signed-zookeeper


# for broker-clients
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:183.95.70.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka

##------------broker2
common name:97.1.122.34.bc.googleusercontent.com
ip:34.122.1.97

# for broker2 and zookeeper connections
keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:localhost
keytool -keystore zookeeper.keystore.jks -alias kafka -certreq -file ca-request-zookeeper
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias kafka -import -file ca-signed-zookeeper

# for broker-clients
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:97.1.122.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka

# for key-store and truststore of zookeeper-client, kafka-admin, consumer,producer
# i will use the old file that were used in the docker setup
# i can do this because i used the same cerificate authority here, as what i had used in case of docker-setup
# if you do not want to do that you can also generate them the same way too.


##------------------------------------------------------------##
                   2. CREATE THE JAAS file
##------------------------------------------------------------##

just copy the one i have used (this jaas file is not used anywhere, it is there just to satisfy the base requirments of the bitnami images)

##-------------------------------------------------------------------------------------------------##
                   3. CREATE THE adequate server.properties and zookeeper.properties file
##-------------------------------------------------------------------------------------------------##

use the uses I have used

##------------------------------------------------------------------------------
Once the requirements are satisfied,

So, we make a Dockerfile for each broker and zookeeper, build the required images and push them to either docker-hub or google registry (later on remember to specify the image location accordingly in the yaml files)

for my particular setup I have done this, for running the custom images on GCP kubernetes engine.
build and push images to GContaner registry using below commands for all zookeepers and brokers
# docker build -t gcr.io/molten-reserve-317003/broker2_image:v1 .
# docker push gcr.io/molten-reserve-317003/broker2_image:v1
similarly for other images


###------------------------------------------------------------------------

Now that the required images are build and are available for use.
We proceed with making the yaml files for brokers and zookeepers.
(use the one that i have used, you can find them in pod-files folder)

###########---------------IMPORTANT-NOTE-----------------#
some properties i have mentioned both in the server.properties file and in the environment variables in yaml files
these properties are then picked up from the mounted server.properties.


########## ----setup in your machine
- update the mapping of IP to DNS name in /etc/hosts file
- this needs to be done for all brokers and all zookeepers

Doing this depends from PC to PC, my linux machine was not able to resolve the DNS, therefore I needed to explicitly mention them in /etc/hosts file

###----------------------------------------------------------------------
Now, all the deployment files and pre-requisites are prepared, lets go the deploying the yaml files.

##-----------NOTE----------------##
For storing the data generated from zookeepers and brokers, i am using persistent disk provided by google cloud storage.
I have used 4 PDs, each of 10GBs one for each component.
-----To create the disks
--$ gcloud compute disks create DISK_NAME --size DISK_SIZE --type DISK_TYPE

### To use the created disk space, we need persistent-Volume and persistent-volume-claims in GCP
Setup of persistent-volume and persistent-volume-claims is present in per-storage-all.yaml file.
(---change the "pd-name" field in per-storage-all.yaml file, accordingly--)

# and now deploy the PV and PVC
kubectl apply -f per-storage-all.yaml

## setting so that pods could attach with the disk, are already there in the yaml files for the zookeeper and broker deployment files.

######---------------LAUNCHING----------------------

For launching the zookeepers and borker, follow the below steps chronologically.

### now lets launch the zookeeper deployment by using zooD0-storage.yaml file
### kubectl  apply -f <filename>
## keep in mind to set the appropriate image name in yaml file

## and look into logs of runnig pod
## kubectl logs -f <pod-name>

Once, the zookeeper is up and running fine. We can proceed

## execut all bin commands from inside kafka_2.13-2.8.0 package(download this package from the link provided at the top of this file)
## For e.g. 
home/mukesh/kafka_2.13-2.8.0~$ <write all commands starting with bin here>

###------check that can connect to zookeeper via zookeeper-shell
bin/zookeeper-shell.sh 155.188.68.34.bc.googleusercontent.com:2182 -zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties


####-----------creating broker account--before deploying the brokers
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name broker-admin --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

###-------Once the broker-account is created, run broker0, broker1, broker2 using the yaml files
### keep in mind the image files and the corressponding env variables
## also the corressponding correct public IP provided by the corresponding already runnig service

## Now, the brokers are up and running,
To conusme and produce messages we need a topic and 
# for creating topics we need a super user
# Creting a super-user account

bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name kafka-admin --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

# Now, for granting the super-user access to the above credential, execute the following ACLs:
# FULL ACCESS for Topics
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation READ --operation WRITE --operation DESCRIBE --operation DESCRIBECONFIGS --operation ALTER --operation ALTERCONFIGS --operation CREATE --operation DELETE --topic '*'

# FULL ACCESS for Groups
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation READ --operation DESCRIBE --operation DELETE --group '*'

# FULL ACCESS for delegation-tokens
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation DESCRIBE --delegation-token '*'

# FULL ACCESS for transactional clients
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation DESCRIBE --operation WRITE --transactional-id '*'

#######....................................................IMPORTANT----------------------------------------------##############3
# find cluster id by doing $ get /cluster/id ---in zookeeper shell ---opened in line 1 on this page
# WMe3OUXVTju93SJ1_Iz6Sg
# and replace it it the below command

# FULL ACCESS to the cluster, replace cluster id in this before use
# bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation ALTER --operation ALTERCONFIGS --operation CLUSTERACTION --operation CREATE --operation DESCRIBE --operation DESCRIBECONFIGS --operation IDEMPOTENTWRITE --cluster 3Umlu_QeSaOCa_fhA3lHYA

Now that the super-user account named kafk-adming is created
### Finally, create the topic using the below command:

bin/kafka-topics.sh --bootstrap-server 234.67.66.34.bc.googleusercontent.com:9092 --command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --create --topic ssl-topic --partitions 2 --replication-factor 3 --config min.insync.replicas=1

## to view the list of topics

bin/kafka-topics.sh --bootstrap-server 34.66.67.234:9092,34.70.95.183:9093,34.122.1.97:9094 -command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --list

### to describe a topic

bin/kafka-topics.sh --bootstrap-server 34.66.67.234:9092,34.70.95.183:9093,34.122.1.97:9094 -command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --describe --topic ssl-topic

#####-----------creating producer account
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name demo-producer --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

####-----------granting producer access to produce to the topic
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-producer --operation WRITE --operation DESCRIBE --operation DESCRIBECONFIGS --topic ssl-topic


####----------creating consumer account
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name demo-consumer --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'


####----------to grant consumer access to consume from the topic
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-consumer --operation READ --operation DESCRIBE --topic ssl-topic


## any consumer by default exits within a consumer group
####---------allowing consumer account to create a consumer group
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-consumer --operation READ --group consumer-group1

### now use the given secure-producer and secure-consumer scripts to produce and consume messages respectively
## to properly use the consumer and producer
## keep in mind the path of ca-cert as mentioned in these scripts
## also the username and password should match as set in this configuration

### for running secure-producer and secure-consumer.py, you would need librdkafka and confluent_kafka packages.
# confluent_kafka is just a wrapper around librdkafka
## also, I have used librdkafka by installing latest from brew, and python 3.93.
## there were some issues with the version compatibility when not using python3.9.(like confluent_kafka was unable to detect librdkafka)