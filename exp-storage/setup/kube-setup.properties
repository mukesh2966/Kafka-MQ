#####_______________SUGGESTION
##### First go through the docker-setup.properties file, to get a better understanding of what's done below.

#############_________________WORK IN PROGRESS____________________________#####

## launch all the 4 services, 3 brokers 1 zookeeper(corresponding yaml files could be found in pod-file folder)

# get their IP, reverse dns lookup their hostname will be required in SSL keystore and truststore generation

## make separate folder for zookeeper ssl files and for brokers ssl files

## use domain name when prompted for first name and last name, while creating keystore
##----------zookeeper

common name:155.188.68.34.bc.googleusercontent.com
ip:34.68.188.155

# for kube setup

keytool -keystore zookeeper.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias zookeeper -validity 3650 -genkey -keyalg RSA -ext SAN=dns:155.188.68.34.bc.googleusercontent.com
keytool -keystore zookeeper.keystore.jks -alias zookeeper -certreq -file ca-request-zookeeper
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-zookeeper -out ca-signed-zookeeper -days 3650 -CAcreateserial
keytool -keystore zookeeper.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore zookeeper.keystore.jks -alias zookeeper -import -file ca-signed-zookeeper


# copy the zookeeper keystore and trustore also to each broker folder

##------------broker0
common name:234.67.66.34.bc.googleusercontent.com
ip:34.66.67.234

# for kube setup
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:234.67.66.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka


##------------broker1
common name:183.95.70.34.bc.googleusercontent.com
ip:34.70.95.183

# for kube setup
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:183.95.70.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka

##------------broker2
common name:97.1.122.34.bc.googleusercontent.com
ip:34.122.1.97

# for kube setup
keytool -keystore kafka.truststore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -validity 3650 -genkey -keyalg RSA -ext SAN=dns:97.1.122.34.bc.googleusercontent.com
keytool -keystore kafka.keystore.jks -alias kafka -certreq -file ca-request-kafka
openssl x509 -req -CA ca-cert -CAkey ca-key -in ca-request-kafka -out ca-signed-kafka -days 3650 -CAcreateserial
keytool -keystore kafka.keystore.jks -alias ca-cert -import -file ca-cert
keytool -keystore kafka.keystore.jks -alias kafka -import -file ca-signed-kafka

# for key-store and truststore of zk-client, kafka-admin, consumer,producer
# i will use the old file
# i can do this because i used the same cerificate authority here, as what i had used in case of docker-setup
# if you do not want to do that you can also generate them the same way too.

#### USEFUL LINKS FOR THE IMAGES USED BELOW:
For zookeeper:
https://hub.docker.com/r/bitnami/zookeeper/

For broker:
https://github.com/bitnami/bitnami-docker-kafka/blob/master/README.md#setting-up-a-kafka-cluster
https://github.com/bitnami/bitnami-docker-kafka/blob/master/2/debian-10/rootfs/opt/bitnami/scripts/libkafka.sh#L221


###--------------------------------- For starting the zookeeper and brokers.--------------------
### now what the idea is that instead of mounting the volumes like in docker-setup.
### we create new images from base bitnami images, with the required mounts already copied to the required folder.

## These three things needed to be copied on top of base image.
### certificates
### kafka_jaas.conf (only for brokers)
### server.properties to the image (only for brokers)
# (For more detail you could check out the respective docker file)

###########---------------IMPORTANT-NOTE-----------------#
## also will need to manually change zookeeper.connect in server.properties before copying to image 
## as bitnami/kafka does not take changes by means of env variables.....
### may be this is because I have directly mounted the server.properties file, instead of using the env variable for setting things up.


####### zookeeper image
# dockerfile created for zookeeper
# copy certificates from /zoo-ssl/ to /bitnami/zookeeper/certs

####### broker0 image
# dockerfile for broker
# copy certificates from /broker0-ssl/ to /bitnami/kafka/config/certs
# COPY server-0.properties /bitnami/kafka/config/server.properties
# also manually changing zookeeper.connect=<DNS of zookeeper>:2182 in server-0.properties
# COPY kafka_jaas.config /opt/bitnami/kafka/config/kafka_jaas.conf

# similarly for broker1, broker2 images

## for my particular setup I have done this, for running the custom images on GCP kubernetes engine.
### you could also push them to docker hub. 
# build and push images to GContaner registry using below commands for all zookeepers and brokers
# docker build -t gcr.io/molten-reserve-317003/broker2_image:1 .
# docker push gcr.io/molten-reserve-317003/broker2_image:1


########## ----setup in your machine
update the mapping of IP to DNS name in /etc/hosts file

this needs to be done for all brokers and all zookeepers

######333---------------LAUNCHING
### now lets launch the zookeeper deployment by using zooD0.yaml file
### kubectl --validate=false apply -f <filename>
## keep in mind to set the appropriate image name in yaml file

## and look into logs of runnig pod
## kubectl logs <pod-name>

## execut all bin commands from inside kafka_2.13-2.8.0 package
## For e.g. 
home/mukesh/kafka_2.13-2.8.0~$ <write all commands starting with bin here>

###------check that can connect to zookeeper via zookeeper-shell
bin/zookeeper-shell.sh 155.188.68.34.bc.googleusercontent.com:2182 -zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties


####-----------creating broker account
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name broker-admin --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

###-------run broker0, broker1, broker2 using the yaml files
### keep in mind the image files and the corressponding env variables
## also the corressponding correct public IP provided by the corresponding already runnig service


# for creating topics we need a super user

bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name kafka-admin --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

# Now, for granting the super-user access to the above credential, execute the following ACLs:
# FULL ACCESS for Topics
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation READ --operation WRITE --operation DESCRIBE --operation DESCRIBECONFIGS --operation ALTER --operation ALTERCONFIGS --operation CREATE --operation DELETE --topic '*'

# FULL ACCESS for Groups
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation READ --operation DESCRIBE --operation DELETE --group '*'

# FULL ACCESS for delegation-tokens
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation DESCRIBE --delegation-token '*'

# FULL ACCESS for transactional clients
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation DESCRIBE --operation WRITE --transactional-id '*'

#######....................................................IMPORTANT----------------------------------------------##############3
# find cluster id by doing $ get /cluster/id ---in zookeeper shell ---opened in line 1 on this page
# WMe3OUXVTju93SJ1_Iz6Sg
# and replace it it the below command

# FULL ACCESS to the cluster, replace cluster id in this before use
# bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:kafka-admin --operation ALTER --operation ALTERCONFIGS --operation CLUSTERACTION --operation CREATE --operation DESCRIBE --operation DESCRIBECONFIGS --operation IDEMPOTENTWRITE --cluster 3Umlu_QeSaOCa_fhA3lHYA


### Finally, create the topic using the below command:

bin/kafka-topics.sh --bootstrap-server 234.67.66.34.bc.googleusercontent.com:9092 --command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --create --topic ssl-topic --partitions 2 --replication-factor 3 --config min.insync.replicas=1

## to view the list of topics

bin/kafka-topics.sh --bootstrap-server 34.66.67.234:9092,34.70.95.183:9093,34.122.1.97:9094 -command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --list

### to describe a topic

bin/kafka-topics.sh --bootstrap-server 34.66.67.234:9092,34.70.95.183:9093,34.122.1.97:9094 -command-config /home/mukesh/kube-demo4/kube-demo/config/kafka-admin.properties --describe --topic ssl-topic

#####-----------creating producer account
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name demo-producer --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'

####-----------granting producer access to produce to the topic
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-producer --operation WRITE --operation DESCRIBE --operation DESCRIBECONFIGS --topic ssl-topic


####----------creating consumer account
bin/kafka-configs.sh --zookeeper 155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --entity-type users --entity-name demo-consumer --alter --add-config 'SCRAM-SHA-512=[password=demokafka]'


####----------to grant consumer access to consume from the topic
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-consumer --operation READ --operation DESCRIBE --topic ssl-topic


## any consumer by default exits within a consumer group
####---------allowing consumer account to create a consumer group
bin/kafka-acls.sh --authorizer-properties zookeeper.connect=155.188.68.34.bc.googleusercontent.com:2182 --zk-tls-config-file /home/mukesh/kube-demo4/kube-demo/config/zookeeper-client.properties --add --allow-principal User:demo-consumer --operation READ --group consumer-group1

### now use the given secure-producer and secure-consumer scripts to produce and consume messages respectively
## to properly use the consumer and producer
## keep in mind the path of ca-cert as mentioned in these scripts
## also the username and password should match as set in this configuration

### for running secure-producer and secure-consumer.py, you would need librdkafka and confluent_kafka packages.
# confluent_kafka is just a wrapper around librdkafka
## also, I have used librdkafka by installing latest from brew, and python 3.93.
## there were some issues with the version compatibility when using these dependencies some other way.(like confluent_kafka was unable to detect librdkafka)




